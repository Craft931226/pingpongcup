{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b9a5ff",
   "metadata": {},
   "source": [
    "### feature_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8930f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy.fft import rfft\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "logging.basicConfig(\n",
    "    filename=\"log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# >>> 常數重整 <<<\n",
    "SEG_DIM      = 335        # extract_features() 單段輸出\n",
    "FATIGUE_DIM  = 102\n",
    "FINAL_DIM    = SEG_DIM + FATIGUE_DIM   # 437\n",
    "# <<< 常數重整 <<<\n",
    "\n",
    "\n",
    "# >>> 官方 27→1 聚合規則 <<<\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_group_prob(proba_mat: np.ndarray, group_size: int = 27) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    將 (N × C) 機率矩陣依官方規則壓縮成 (N/27 × C)：\n",
    "    1. 先把 27 次機率「逐類別加總」\n",
    "    2. 找加總值最大的類別 major_cls\n",
    "    3. 在這 27 次裡挑出 major_cls 機率最高的那一次\n",
    "    4. 取那一次的整排機率當此檔最終輸出\n",
    "    \"\"\"\n",
    "    if proba_mat.size == 0:\n",
    "        return np.array([[0.]])\n",
    "\n",
    "    # 如果長度不足一組 27，就直接用同一規則處理整段\n",
    "    if len(proba_mat) < group_size:\n",
    "        proba_mat = proba_mat.reshape(1, len(proba_mat), -1)\n",
    "    else:\n",
    "        total      = (len(proba_mat) // group_size) * group_size\n",
    "        proba_mat  = proba_mat[:total].reshape(-1, group_size, proba_mat.shape[1])\n",
    "\n",
    "    agg_list = []\n",
    "    for grp in proba_mat:                     # grp.shape = (27, C)\n",
    "        sum_prob   = grp.sum(axis=0)          # (C,)\n",
    "        major_cls  = sum_prob.argmax()\n",
    "        best_idx   = grp[:, major_cls].argmax()\n",
    "        agg_list.append(grp[best_idx])        # (C,)\n",
    "\n",
    "    return np.stack(agg_list, axis=0)\n",
    "# <<< 官方 27→1 聚合規則 >>>\n",
    "\n",
    "\n",
    "    \n",
    "def FFT_data(input_data, swinging_times):   \n",
    "    txtlength = swinging_times[-1] - swinging_times[0]\n",
    "    a_mean = [0] * txtlength\n",
    "    g_mean = [0] * txtlength\n",
    "       \n",
    "    for num in range(len(swinging_times)-1):\n",
    "        a = []\n",
    "        g = []\n",
    "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
    "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
    "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
    "        a_mean[num] = (sum(a) / len(a))\n",
    "        g_mean[num] = (sum(g) / len(g))\n",
    "    \n",
    "    return a_mean, g_mean\n",
    "\n",
    "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
    "    # Convert input data to numpy array\n",
    "    arr = np.array(input_data)\n",
    "    \n",
    "    if swinging_times == 0:  # Handle case where there are no swings\n",
    "        swinging_times = 1\n",
    "        \n",
    "    # Calculate frequency domain features\n",
    "    cut = int(n_fft / swinging_times)\n",
    "    idx_start = cut * swinging_now\n",
    "    idx_end = min(cut * (swinging_now + 1), len(a_fft))  # Ensure we don't go out of bounds\n",
    "    \n",
    "    # Rest of the function remains the same...\n",
    "    # Calculate acceleration and gyroscope vectors\n",
    "    a_vec = np.sqrt(np.sum(arr[:, :3]**2, axis=1))\n",
    "    g_vec = np.sqrt(np.sum(arr[:, 3:6]**2, axis=1))\n",
    "    \n",
    "    a_stats = [a_vec.max(), a_vec.mean(), a_vec.min()]\n",
    "    g_stats = [g_vec.max(), g_vec.mean(), g_vec.min()]\n",
    "    \n",
    "    # Calculate kurtosis and skewness\n",
    "    a_centered = a_vec - a_vec.mean()\n",
    "    g_centered = g_vec - g_vec.mean()\n",
    "    \n",
    "    a_moments = {\n",
    "        'skew': np.mean(a_centered**3) / (np.std(a_centered)**3),\n",
    "        'kurt': np.mean(a_centered**4) / (np.var(a_centered)**2)\n",
    "    }\n",
    "    g_moments = {\n",
    "        'skew': np.mean(g_centered**3) / (np.std(g_centered)**3),\n",
    "        'kurt': np.mean(g_centered**4) / (np.var(g_centered)**2)\n",
    "    }\n",
    "    \n",
    "    # Get FFT slices safely\n",
    "    a_fft_slice = a_fft[idx_start:idx_end]\n",
    "    g_fft_slice = g_fft[idx_start:idx_end]\n",
    "    a_fft_imag_slice = a_fft_imag[idx_start:idx_end]\n",
    "    g_fft_imag_slice = g_fft_imag[idx_start:idx_end]\n",
    "    \n",
    "    # Handle empty slices\n",
    "    if len(a_fft_slice) == 0:\n",
    "        a_fft_slice = np.array([0])\n",
    "        g_fft_slice = np.array([0])\n",
    "        a_fft_imag_slice = np.array([0])\n",
    "        g_fft_imag_slice = np.array([0])\n",
    "    \n",
    "    # Calculate PSD using vectorized operations\n",
    "    a_psd = np.power(a_fft_slice, 2) + np.power(a_fft_imag_slice, 2)\n",
    "    g_psd = np.power(g_fft_slice, 2) + np.power(g_fft_imag_slice, 2)\n",
    "    \n",
    "    # Calculate entropy with safety checks\n",
    "    e1 = np.sqrt(a_psd)\n",
    "    e3 = np.sqrt(g_psd)\n",
    "    e2 = np.sum(e1) + 1e-10  # Avoid division by zero\n",
    "    e4 = np.sum(e3) + 1e-10\n",
    "    \n",
    "    p_a = e1 / e2\n",
    "    p_g = e3 / e4\n",
    "    entropy_a = np.sum(p_a * np.log(p_a + 1e-10)) / max(cut, 1)\n",
    "    entropy_g = np.sum(p_g * np.log(p_g + 1e-10)) / max(cut, 1)\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    mean = np.mean(arr, axis=0)\n",
    "    std = np.std(arr, axis=0)\n",
    "    rms = np.sqrt(np.mean(arr**2, axis=0))\n",
    "    \n",
    "    # Combine all features\n",
    "    output = np.concatenate([\n",
    "        mean, std, rms,\n",
    "        a_stats, g_stats,\n",
    "        [np.mean(a_fft_slice), np.mean(g_fft_slice)],\n",
    "        [np.mean(a_psd), np.mean(g_psd)],\n",
    "        [a_moments['kurt'], g_moments['kurt']],\n",
    "        [a_moments['skew'], g_moments['skew']],\n",
    "        [entropy_a, entropy_g]\n",
    "    ]).tolist()\n",
    "    \n",
    "    writer.writerow(output)\n",
    "\n",
    "def extract_features(df):\n",
    "    features = []\n",
    "    \n",
    "    # 速度相關特徵\n",
    "    speed_cols = ['Ax', 'Ay', 'Az']\n",
    "    speeds = df[speed_cols].values\n",
    "    \n",
    "    # 基礎統計特徵\n",
    "    features.extend([\n",
    "        np.mean(speeds, axis=0),\n",
    "        np.std(speeds, axis=0),\n",
    "        np.max(speeds, axis=0),\n",
    "        np.min(speeds, axis=0),\n",
    "        np.percentile(speeds, 25, axis=0),\n",
    "        np.percentile(speeds, 75, axis=0),\n",
    "        scipy.stats.skew(speeds, axis=0),\n",
    "        scipy.stats.kurtosis(speeds, axis=0)\n",
    "    ])\n",
    "    # --- Δ / Δ² 特徵：捕捉加速度變化趨勢 ---\n",
    "    delta1 = np.diff(speeds, axis=0, n=1)   # 一階差分\n",
    "    delta2 = np.diff(speeds, axis=0, n=2)   # 二階差分\n",
    "    if delta1.size == 0:\n",
    "        delta1 = np.zeros((1, 3))\n",
    "    if delta2.size == 0:\n",
    "        delta2 = np.zeros((1, 3))\n",
    "    features.extend([\n",
    "        np.mean(delta1, axis=0), np.std(delta1, axis=0),\n",
    "        np.mean(delta2, axis=0), np.std(delta2, axis=0),\n",
    "    ])\n",
    "    # 速度交互特徵\n",
    "    speed_means = np.mean(speeds, axis=0)\n",
    "    features.extend([\n",
    "        speed_means[0] * speed_means[1],  # x * y\n",
    "        speed_means[1] * speed_means[2],  # y * z\n",
    "        speed_means[0] * speed_means[2]   # x * z\n",
    "    ])\n",
    "    \n",
    "    # 加速度特徵\n",
    "    acc = np.diff(speeds, axis=0)\n",
    "    features.extend([\n",
    "        np.mean(acc, axis=0),\n",
    "        np.std(acc, axis=0),\n",
    "        np.max(acc, axis=0),\n",
    "        np.min(acc, axis=0)\n",
    "    ])\n",
    "    \n",
    "    # 位置相關特徵\n",
    "    pos_cols   = ['Gx', 'Gy', 'Gz']\n",
    "    positions = df[pos_cols].values\n",
    "    \n",
    "    # 位置統計特徵\n",
    "    features.extend([\n",
    "        np.mean(positions, axis=0),\n",
    "        np.std(positions, axis=0),\n",
    "        np.max(positions, axis=0),\n",
    "        np.min(positions, axis=0),\n",
    "        np.percentile(positions, 25, axis=0),\n",
    "        np.percentile(positions, 75, axis=0)\n",
    "    ])\n",
    "    \n",
    "    # 位置變化特徵\n",
    "    pos_diff = np.diff(positions, axis=0)\n",
    "    features.extend([\n",
    "        np.mean(pos_diff, axis=0),\n",
    "        np.std(pos_diff, axis=0)\n",
    "    ])\n",
    "    \n",
    "    # 時序特徵\n",
    "    for col in speed_cols + pos_cols:\n",
    "        #\n",
    "        ts = df[col].values\n",
    "        # FFT特徵\n",
    "        n = len(ts)\n",
    "        window = np.hanning(n)                    # 1. 建立窗函數\n",
    "        ts_windowed = ts * window                 # 2. 乘上窗\n",
    "        n_fft = 1 << (n - 1).bit_length()         # 3. 計算下個 2 的冪次\n",
    "        ts_padded = np.pad(ts_windowed,           # 4. 以零補齊\n",
    "                        (0, n_fft - n),\n",
    "                        mode='constant')\n",
    "        fft_features = np.abs(np.fft.fft(ts_padded))[:5]\n",
    "        fft_features = np.abs(np.fft.fft(ts))[:5]  # 取前5個頻率分量\n",
    "        features.extend(fft_features)\n",
    "        \n",
    "        # 自相關特徵\n",
    "        acf = np.correlate(ts, ts, mode='full') / len(ts)\n",
    "        features.extend(acf[len(acf)//2:len(acf)//2+3])  # 取中心點後3個值\n",
    "    \n",
    "    # 三維向量特徵\n",
    "    speed_magnitudes = np.linalg.norm(speeds, axis=1)\n",
    "    pos_magnitudes = np.linalg.norm(positions, axis=1)\n",
    "    \n",
    "    features.extend([\n",
    "        np.mean(speed_magnitudes),\n",
    "        np.std(speed_magnitudes),\n",
    "        np.max(speed_magnitudes),\n",
    "        np.min(speed_magnitudes),\n",
    "        np.mean(pos_magnitudes),\n",
    "        np.std(pos_magnitudes),\n",
    "        np.max(pos_magnitudes),\n",
    "        np.min(pos_magnitudes)\n",
    "    ])\n",
    "    \n",
    "    # --- 把 features 轉成 1-D list ---\n",
    "    flat = []\n",
    "    for f in features:\n",
    "        if isinstance(f, np.ndarray):\n",
    "            flat.extend(f.ravel())        # array(3,) → 3 個值\n",
    "        else:                             # Python float / int / scalar\n",
    "            flat.append(float(f))\n",
    "\n",
    "    flat = np.asarray(flat, dtype=np.float32)\n",
    "\n",
    "    # ---- 保證長度一致：不足補 0，多餘截斷 ----\n",
    "    if flat.size < SEG_DIM:\n",
    "        flat = np.pad(flat, (0, SEG_DIM - flat.size))\n",
    "    elif flat.size > SEG_DIM:\n",
    "        flat = flat[:SEG_DIM]\n",
    "\n",
    "\n",
    "    return flat\n",
    "\n",
    "# >>> 新增 疲勞/穩定度特徵 Helper Function <<<\n",
    "def compute_fatigue_features(segments):\n",
    "    \"\"\"\n",
    "    6 個通道 × 17 指標 ＝ 102 維\n",
    "    指標 = 4 條序列(max/mean/std/p90) 的\n",
    "           slope‧intercept‧R²‧(last-first)  +  std_ratio\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    chs  = ['Ax', 'Ay', 'Az', 'Gx', 'Gy', 'Gz']\n",
    "    rng  = np.arange(len(segments))          # 0…26\n",
    "    eps  = 1e-6                              # 極小值，避免除零\n",
    "    feat = []\n",
    "\n",
    "    for c in chs:\n",
    "        max_s  = [seg[c].abs().max()              for seg in segments]\n",
    "        mean_s = [seg[c].abs().mean()             for seg in segments]\n",
    "        std_s  = [seg[c].std()                    for seg in segments]\n",
    "        p90_s  = [np.percentile(seg[c].abs(), 90) for seg in segments]\n",
    "\n",
    "        for series in (max_s, mean_s, std_s, p90_s):\n",
    "            # 若序列全相同 → 標準差為 0，直接設定斜率、R² 為 0\n",
    "            if np.std(series) < eps:\n",
    "                k, b, r2 = 0.0, series[0], 0.0\n",
    "            else:\n",
    "                k, b = np.polyfit(rng, series, 1)\n",
    "                r2   = np.corrcoef(rng, series)[0, 1]**2\n",
    "            feat.extend([k, b, r2, series[-1] - series[0]])\n",
    "\n",
    "        std_ratio = (np.mean(std_s[:13]) + eps) / (np.mean(std_s[13:]) + eps)\n",
    "        feat.append(std_ratio)\n",
    "\n",
    "    # 確保沒有 nan / inf\n",
    "    return np.nan_to_num(np.array(feat), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "# <<< 新增 疲勞/穩定度特徵 Helper Function >>>\n",
    "\n",
    "\n",
    "\n",
    "def generate_features(raw_dir: str, info_csv:str,  out_dir: str):\n",
    "    # 讀取 cut_point；用 unique_id 當 index 方便隨查\n",
    "    info_df = pd.read_csv(info_csv).set_index(\"unique_id\")\n",
    "\n",
    "    Path(out_dir).mkdir(exist_ok=True)\n",
    "    pathlist_txt = Path(raw_dir).glob('*.txt')\n",
    "\n",
    "    for file in pathlist_txt:\n",
    "        # 讀取文件\n",
    "        data = []\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f.readlines()[1:]:  # Skip header\n",
    "                if line.strip():  # Skip empty lines\n",
    "                    values = line.strip().split()\n",
    "                    if len(values) >= 6:\n",
    "                        # 只取前6個值，並轉換為整數\n",
    "                        row = [int(x) for x in values[:6]]\n",
    "                        data.append(row)\n",
    "\n",
    "        if not data:\n",
    "            print(f\"Warning: No valid data found in {file}\")\n",
    "            logging.warning(f\"No valid data found in {file}\")\n",
    "            continue\n",
    "\n",
    "        # 創建DataFrame並命名列\n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            'Ax', 'Ay', 'Az',    # Accelerometer\n",
    "            'Gx', 'Gy', 'Gz'     # Gyroscope\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # --- 依 cut_point 精準切 27 段；缺切點則回退等分 ---\n",
    "            uid        = int(file.stem)\n",
    "            try:\n",
    "               cuts_raw = info_df.loc[uid, \"cut_point\"]          # \"[0 35 ... 998]\"\n",
    "               cuts     = np.fromstring(cuts_raw.strip(\"[]\"), sep=\" \", dtype=int)\n",
    "               assert len(cuts) == 28\n",
    "               segments  = [df.iloc[cuts[i]:cuts[i+1]] for i in range(27)]\n",
    "            except Exception as _:\n",
    "               # 若該檔案沒有 cut_point 或解析失敗 → 回退均分\n",
    "               idx_splits = np.array_split(np.arange(len(df)), 27)\n",
    "               segments   = [df.iloc[idx] for idx in idx_splits]\n",
    "            seg_feats = [extract_features(seg) for seg in segments]\n",
    "\n",
    "            # 🟢 確認所有段落向量長度一致，否則捨棄異常段\n",
    "            base_len = len(seg_feats[0])\n",
    "            seg_feats = [v for v in seg_feats if len(v) == base_len]\n",
    "            if len(seg_feats) == 0:                 # 全部失敗 → 填 0\n",
    "                seg_feats = [np.zeros(base_len)]\n",
    "            # 這裡示範取平均；想取最大值可改 np.max(seg_feats, axis=0)\n",
    "            fatigue_vec = compute_fatigue_features(segments)\n",
    "            features = np.concatenate([np.mean(seg_feats, axis=0), fatigue_vec])\n",
    "            assert features.shape[0] == FINAL_DIM, \\\n",
    "                f\"feature dim={features.shape[0]}, expect {FINAL_DIM}\"\n",
    "            \n",
    "            # 保存特徵\n",
    "            features_df = pd.DataFrame([features])\n",
    "            output_path = Path(out_dir) / f\"{file.stem}.csv\"\n",
    "            features_df.to_csv(output_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c2b72",
   "metadata": {},
   "source": [
    "### model_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import logging\n",
    "import joblib\n",
    "#  lightgbm as lgb\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# from sklearn.calibration import label_binarize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "TARGETS = {\n",
    "    \"gender\":             {\"type\": \"bin\", \"name\": \"gender\"},\n",
    "    \"hold racket handed\": {\"type\": \"bin\", \"name\": \"hold\"},\n",
    "    \"play years\":         {\"type\": \"multi\", \"num_class\": 3, \"name\": \"play_years\"},\n",
    "    \"level\":              {\"type\": \"multi\", \"num_class\": 4, \"name\": \"level\"},\n",
    "}\n",
    "\n",
    "def build_scaler(X: np.ndarray):\n",
    "    sc = MinMaxScaler()\n",
    "    return sc.fit(X)\n",
    "\n",
    "def build_model(y, meta):\n",
    "    \"\"\"\n",
    "    回傳一個 RNN 模型\n",
    "    y: 1D numpy array (labels)\n",
    "    meta: dict，含 'num_classes' 或 binary\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.metrics import AUC as AUCMetric\n",
    "    # 1. 轉為 one-hot（若 multi-class）\n",
    "    num_classes = meta.get(\"num_classes\", None)\n",
    "    if num_classes and num_classes > 2:\n",
    "        y_cat = to_categorical(y, num_classes)\n",
    "        loss = \"categorical_crossentropy\"\n",
    "        output_units = num_classes\n",
    "        activation = \"softmax\"\n",
    "    else:\n",
    "        y_cat = y\n",
    "        loss = \"binary_crossentropy\"\n",
    "        output_units = 1\n",
    "        activation = \"sigmoid\"\n",
    "\n",
    "    # 2. 建模型\n",
    "    model = Sequential([\n",
    "        # 輸入 shape 會在 main.py 傳入\n",
    "        LSTM(64, return_sequences=True, input_shape=(None, meta[\"n_features\"])),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(output_units, activation=activation)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=loss, metrics=[AUCMetric(name=\"auc\")])\n",
    "    # 包成一個 dict，保留給外部呼叫\n",
    "    return {\"model\": model, \"y_cat\": y_cat}\n",
    "\n",
    "\n",
    "def cv_evaluate(model, X, y, groups, target_info, early_stopping_rounds=30):\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = []\n",
    "\n",
    "    for tr_idx, val_idx in gkf.split(X, y, groups):\n",
    "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
    "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "        \n",
    "        eval_set = [(X_val, y_val)]\n",
    "        \n",
    "        # Use different metrics for binary and multiclass\n",
    "        eval_metric = 'auc' if target_info[\"type\"] == \"bin\" else 'multi_logloss'\n",
    "        \n",
    "        model.fit(X_tr, y_tr,\n",
    "                 eval_set=eval_set,\n",
    "                 eval_metric=eval_metric,\n",
    "                 callbacks=[lgb.early_stopping(early_stopping_rounds)])\n",
    "                 \n",
    "        proba = model.predict_proba(X_val)\n",
    "\n",
    "        # ---------- Binary ----------\n",
    "        if target_info[\"type\"] == \"bin\":\n",
    "            pos_prob = proba[:, 1] if proba.ndim == 2 else proba.ravel()\n",
    "            scores.append(roc_auc_score(y_val, pos_prob))\n",
    "            continue\n",
    "\n",
    "        # ---------- Multi-class ----------\n",
    "        present = np.unique(y_val)             \n",
    "        if len(present) == 1:                       \n",
    "            continue                                \n",
    "\n",
    "        col_of = {c: i for i, c in enumerate(model.classes_)}\n",
    "        \n",
    "        if len(present) == 2:\n",
    "            pos_cls   = present[1]\n",
    "            pos_prob  = proba[:, col_of[pos_cls]]\n",
    "            y_bin = (y_val == pos_cls).astype(int)   \n",
    "            score = roc_auc_score(y_bin, pos_prob)\n",
    "            scores.append(score)\n",
    "            continue\n",
    "\n",
    "        proba_use = proba[:, [col_of[c] for c in present]]\n",
    "\n",
    "        if proba_use.ndim > 1:\n",
    "            proba_use = proba_use / proba_use.sum(axis=1, keepdims=True)\n",
    "\n",
    "        score = roc_auc_score(\n",
    "            y_val, proba_use,\n",
    "            labels=present, average=\"micro\", multi_class=\"ovr\"\n",
    "        )\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores) if scores else 0.5\n",
    "\n",
    "def save_model(model, scaler, feature_names, col):      # ← 多一個參數\n",
    "    Path(\"models\").mkdir(exist_ok=True, parents=True)\n",
    "    bundle = {                                         # ★ 多存 feature_names\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "    joblib.dump(bundle, f\"./models/{col}.pkl\")\n",
    "\n",
    "def load_model(name):\n",
    "    return joblib.load(f\"models/{name}.pkl\")\n",
    "\n",
    "def tune_lgb_params(X_train, y_train, groups, target_info, n_trials: int = 30):\n",
    "    \"\"\"使用 Optuna 做 GroupKFold 的 LGB 超參數優化，回傳 best_params\"\"\"\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            \"objective\": \"binary\" if target_info[\"type\"]==\"bin\" else \"multiclass\",\n",
    "            \"metric\":    \"auc\"    if target_info[\"type\"]==\"bin\" else \"multi_logloss\",\n",
    "            \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-1),\n",
    "            \"num_leaves\":    trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            \"subsample\":         trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\":  trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\":         trial.suggest_loguniform(\"reg_alpha\", 1e-3, 10),\n",
    "            \"reg_lambda\":        trial.suggest_loguniform(\"reg_lambda\", 1e-3, 10),\n",
    "            \"n_estimators\": 1000,\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\":       -1,\n",
    "        }\n",
    "        cv = GroupKFold(n_splits=3)\n",
    "        scores = []\n",
    "        for tr_idx, val_idx in cv.split(X_train, y_train, groups):\n",
    "            X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "            if target_info[\"type\"] == \"multi\" and len(np.unique(y_tr)) < target_info[\"num_class\"]:\n",
    "                return 0.0\n",
    "            try:\n",
    "                clf = lgb.LGBMClassifier(**param)\n",
    "                clf.fit(\n",
    "                    X_tr, y_tr,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    eval_metric=param[\"metric\"],\n",
    "                    callbacks=[lgb.early_stopping(30)]\n",
    "                )\n",
    "                proba = clf.predict_proba(X_val)\n",
    "            except Exception:\n",
    "                # 包含 unseen labels 或其他 fit 錯誤都視為此 trial 失敗\n",
    "                return 0.0\n",
    "            try:\n",
    "                if target_info[\"type\"] == \"bin\":\n",
    "                    score = roc_auc_score(y_val, proba[:, 1])\n",
    "                else:\n",
    "                    import pandas as pd\n",
    "                    y_ohe = pd.get_dummies(y_val)\n",
    "                    score = roc_auc_score(\n",
    "                        y_ohe, proba,\n",
    "                        multi_class=\"ovr\", average=\"micro\"\n",
    "                    )\n",
    "                if np.isnan(score):\n",
    "                    return 0.5\n",
    "                scores.append(score)\n",
    "            except ValueError:\n",
    "                # Handle the case where all labels are the same\n",
    "                return 0.5\n",
    "        return sum(scores) / len(scores)\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    if any(t.state == TrialState.COMPLETE for t in study.trials):\n",
    "        return study.best_trial.params\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed99ac5",
   "metadata": {},
   "source": [
    "### train_val_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd52d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data leakage detected.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# import lightgbm as lgb\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def stacking_predict(models_dict, X_val, meta_list):\n",
    "    \"\"\"取所有基模型機率當特徵，訓練簡單 LR 做二階融合\"\"\"\n",
    "    # 第一層預測\n",
    "    layer1 = []\n",
    "    for name, bundle in models_dict.items():\n",
    "        proba = bundle[\"model\"].predict_proba(X_val)\n",
    "        layer1.append(proba)\n",
    "    X_stack = np.hstack(layer1)\n",
    "    # 用真實 y 建 LR\n",
    "    true_y = np.column_stack([meta_list[t] for t in models_dict.keys()])\n",
    "    # 這裡示範單目標，實際可延伸\n",
    "    lr = LogisticRegression(max_iter=500)\n",
    "    lr.fit(X_stack, true_y.ravel())\n",
    "    return lr, X_stack\n",
    "\n",
    "def train_validate_split(X, y, groups, test_size=0.2, random_state=42):\n",
    "    from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "    # Log the length of groups before splitting\n",
    "    # print(f\"Total number of unique groups: {len(groups)}\")\n",
    "\n",
    "    # Debug: Print the first few values of groups\n",
    "    # print(f\"First 10 values in groups: {groups[:10]}\")\n",
    "\n",
    "    # Check unique values in groups\n",
    "    unique_groups, group_counts = np.unique(groups, return_counts=True)\n",
    "    # print(f\"Number of unique groups: {len(unique_groups)}\")\n",
    "    # logging.info(f\"Number of unique groups: {len(unique_groups)}\")\n",
    "    # print(f\"Group counts: {dict(zip(unique_groups, group_counts))}\")\n",
    "    # logging.info(f\"Group counts: {dict(zip(unique_groups, group_counts))}\")\n",
    "\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, val_idx = next(splitter.split(X, y, groups))\n",
    "\n",
    "    # Ensure unique_id is not repeated\n",
    "    train_unique_ids = np.unique(groups[train_idx])\n",
    "    val_unique_ids = np.unique(groups[val_idx])\n",
    "\n",
    "    if set(train_unique_ids).intersection(set(val_unique_ids)):\n",
    "        raise ValueError(\"Data leakage detected: Some unique_ids are in both training and validation sets.\")\n",
    "\n",
    "    # Save unique_ids for debugging\n",
    "    np.savetxt(\"train_ids_from_split.txt\", train_unique_ids, fmt=\"%d\")\n",
    "    np.savetxt(\"val_ids_from_split.txt\", val_unique_ids, fmt=\"%d\")\n",
    "\n",
    "    return {\n",
    "        'X_train': X[train_idx],\n",
    "        'y_train': y[train_idx],\n",
    "        'groups_train': groups[train_idx],\n",
    "        'X_val': X[val_idx],\n",
    "        'y_val': y[val_idx],\n",
    "        'groups_val': groups[val_idx]\n",
    "    }\n",
    "\n",
    "def check_data_leakage(train_ids, val_ids):\n",
    "    \"\"\"Check for data leakage between training and validation datasets.\"\"\"\n",
    "    train_set = set(train_ids)\n",
    "    val_set = set(val_ids)\n",
    "\n",
    "    # Find intersection\n",
    "    leakage = train_set.intersection(val_set)\n",
    "    if leakage:\n",
    "        print(\"Data leakage detected! Overlapping IDs:\", leakage)\n",
    "        logging.error(f\"Data leakage detected! Overlapping IDs: {leakage}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No data leakage detected.\")\n",
    "        logging.info(\"No data leakage detected.\")\n",
    "        return False\n",
    "\n",
    "# def evaluate_model(model, data_dict, target_info):\n",
    "#     \"\"\"Evaluate model performance on validation set\"\"\"\n",
    "#     model.fit(\n",
    "#         data_dict['X_train'], \n",
    "#         data_dict['y_train'],\n",
    "#         eval_set=[(data_dict['X_val'], data_dict['y_val'])],\n",
    "#         eval_metric='auc' if target_info[\"type\"] == \"bin\" else 'multi_logloss',\n",
    "#         callbacks=[lgb.early_stopping(50)]\n",
    "#     )\n",
    "    \n",
    "#     proba = model.predict_proba(data_dict['X_val'])\n",
    "    \n",
    "#     if target_info[\"type\"] == \"bin\":\n",
    "#         score = roc_auc_score(data_dict['y_val'], proba[:, 1])\n",
    "#     else:\n",
    "#         # Convert validation labels to one-hot encoding\n",
    "#         classes = np.unique(data_dict['y_train'])\n",
    "#         y_val_onehot = np.zeros((len(data_dict['y_val']), len(classes)))\n",
    "#         for i, cls in enumerate(classes):\n",
    "#             y_val_onehot[:, i] = (data_dict['y_val'] == cls).astype(int)\n",
    "            \n",
    "#         score = roc_auc_score(\n",
    "#             y_val_onehot,\n",
    "#             proba,\n",
    "#             multi_class=\"ovr\", \n",
    "#             average=\"micro\"\n",
    "#         )\n",
    "    \n",
    "#     return score, model\n",
    "\n",
    "\n",
    "def evaluate_validation_set(data_dict, models_dict, target_info):\n",
    "    \"\"\"Calculate ROC AUC scores for validation data using same logic as evaluate_predictions\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    for target_name, meta in target_info.items():\n",
    "        model = models_dict[target_name]['model']\n",
    "        scaler = models_dict[target_name]['scaler']\n",
    "        X_val_scaled = scaler.transform(data_dict['X_val'])\n",
    "        proba = model.predict_proba(X_val_scaled)\n",
    "        \n",
    "        if meta[\"type\"] == \"bin\":\n",
    "            # 二元分類 - 轉換為 0/1\n",
    "            true_vals = (data_dict['y_val'][target_name] == 1).astype(int)\n",
    "            pred_vals = proba[:, 1]  # 使用正類的概率\n",
    "            score = roc_auc_score(true_vals, pred_vals)\n",
    "        else:\n",
    "            # 多分類 - 使用 one-hot 編碼\n",
    "            if target_name == \"play years\":\n",
    "                true_vals = pd.get_dummies(data_dict['y_val'][target_name])\n",
    "                pred_vals = pd.DataFrame(proba, columns=range(3))\n",
    "            else:  # level\n",
    "                true_vals = pd.get_dummies(data_dict['y_val'][target_name])\n",
    "                pred_vals = pd.DataFrame(proba, columns=[2,3,4,5])\n",
    "            \n",
    "            score = roc_auc_score(\n",
    "                true_vals, pred_vals,\n",
    "                multi_class=\"ovr\",\n",
    "                average=\"micro\"\n",
    "            )\n",
    "            \n",
    "        scores[target_name] = score\n",
    "        print(f\"{target_name} ROC AUC: {score:.4f}\")\n",
    "        logging.info(f\"{target_name} ROC AUC: {score:.4f}\")\n",
    "    \n",
    "    avg_score = np.mean(list(scores.values()))\n",
    "    print(f\"\\nAverage ROC AUC: {avg_score:.4f}\")\n",
    "    logging.info(f\"Average ROC AUC: {avg_score:.4f}\")\n",
    "    return scores, avg_score\n",
    "\n",
    "def check_unique_id_overlap(train_file, val_file):\n",
    "    \"\"\"Check if any unique_id exists in both train and validation files.\"\"\"\n",
    "    with open(train_file, 'r') as f:\n",
    "        train_ids = set(map(int, f.readlines()))\n",
    "\n",
    "    with open(val_file, 'r') as f:\n",
    "        val_ids = set(map(int, f.readlines()))\n",
    "\n",
    "    overlap = train_ids.intersection(val_ids)\n",
    "    if overlap:\n",
    "        print(\"Data leakage detected! Overlapping unique_ids:\", overlap)\n",
    "        logging.error(f\"Data leakage detected! Overlapping unique_ids: {overlap}\")\n",
    "    else:\n",
    "        print(\"No data leakage detected.\")\n",
    "        logging.info(\"No data leakage detected.\")\n",
    "\n",
    "# Example usage\n",
    "check_unique_id_overlap(\"train_ids_from_split.txt\", \"val_ids_from_split.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9ea06",
   "metadata": {},
   "source": [
    "### main.py\n",
    "prepare_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b71a8d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for gender:\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 138\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;66;03m# mdl = build_model(y, meta)\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# if holdout_data is None:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# np.save(\"split_uid_val.npy\", \u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m#         np.unique(uid_idx[np.isin(groups, holdout_data['groups_val'])]))\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Models saved to ./models/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 138\u001b[0m \u001b[43mprepare_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 97\u001b[0m, in \u001b[0;36mprepare_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m y_tr \u001b[38;5;241m=\u001b[39m y_cat[:split] \u001b[38;5;28;01mif\u001b[39;00m y_cat\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y[:split]\n\u001b[0;32m     96\u001b[0m y_val \u001b[38;5;241m=\u001b[39m y_cat[split:] \u001b[38;5;28;01mif\u001b[39;00m y_cat\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y[split:]        \u001b[38;5;66;03m# 開始訓練\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[],  # 若要 early stopping 可加\u001b[39;49;00m\n\u001b[0;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# 驗證結果\u001b[39;00m\n\u001b[0;32m    105\u001b[0m val_auc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_auc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    219\u001b[0m     ):\n\u001b[1;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1059\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1060\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:133\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[1;32m--> 133\u001b[0m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     empty_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:906\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[1;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    910\u001b[0m   bound_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\n\u001b[0;32m    911\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds\n\u001b[0;32m    912\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1060\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1057\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1059\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1060\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: from cache\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx()\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m ag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph is disabled in context\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:114\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    116\u001b[0m         outputs,\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    118\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3261\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4060\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 4061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:85\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model does not have any trainable weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\trainers\\trainer.py:490\u001b[0m, in \u001b[0;36mTrainer.compute_metrics\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m x  \u001b[38;5;66;03m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics_result()\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:334\u001b[0m, in \u001b[0;36mCompileMetrics.update_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m, y_t, y_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_metrics, y_true, y_pred):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m m:\n\u001b[1;32m--> 334\u001b[0m         \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_y(sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\trainers\\compile_utils.py:21\u001b[0m, in \u001b[0;36mMetricsList.update_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n\u001b[1;32m---> 21\u001b[0m         \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\metrics\\confusion_metrics.py:1376\u001b[0m, in \u001b[0;36mAUC.update_state\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_logits:\n\u001b[0;32m   1374\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39msigmoid(y_pred)\n\u001b[1;32m-> 1376\u001b[0m \u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_confusion_matrix_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfusionMatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRUE_POSITIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_positives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfusionMatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRUE_NEGATIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfusionMatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFALSE_POSITIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfalse_positives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConfusionMatrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFALSE_NEGATIVES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfalse_negatives\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthresholds_distributed_evenly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thresholds_distributed_evenly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py:481\u001b[0m, in \u001b[0;36mupdate_confusion_matrix_variables\u001b[1;34m(variables_to_update, y_true, y_pred, thresholds, top_k, class_id, sample_weight, multi_label, label_weights, thresholds_distributed_evenly)\u001b[0m\n\u001b[0;32m    478\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, class_id, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresholds_distributed_evenly:\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_update_confusion_matrix_variables_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariables_to_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthresholds_with_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthresholds_with_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m y_pred\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    493\u001b[0m     pred_shape \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\metrics\\metrics_utils.py:238\u001b[0m, in \u001b[0;36m_update_confusion_matrix_variables_optimized\u001b[1;34m(variables_to_update, y_true, y_pred, thresholds, multi_label, sample_weights, label_weights, thresholds_with_epsilon)\u001b[0m\n\u001b[0;32m    229\u001b[0m bucket_indices \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    230\u001b[0m     ops\u001b[38;5;241m.\u001b[39mceil(y_pred \u001b[38;5;241m*\u001b[39m (ops\u001b[38;5;241m.\u001b[39mcast(num_thresholds, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    232\u001b[0m )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresholds_with_epsilon:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# In this case, the first bucket should actually take into account since\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# the any prediction between [0.0, 1.0] should be larger than the first\u001b[39;00m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;66;03m# threshold. We change the bucket value from -1 to 0.\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m     bucket_indices \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m bucket_indices \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(bucket_indices, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_label:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# We need to run bucket segment sum for each of the label class. In the\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# multi_label case, the rank of the label is 2. We first transpose it so\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# that the label dim becomes the first and we can parallel run though\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;66;03m# them.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\ops\\nn.py:47\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Relu()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:15\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrelu\u001b[39m(x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:11612\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(features, name)\u001b[0m\n\u001b[0;32m  11610\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m  11611\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 11612\u001b[0m   _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11613\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11614\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m  11615\u001b[0m   _result \u001b[38;5;241m=\u001b[39m _dispatch\u001b[38;5;241m.\u001b[39mdispatch(\n\u001b[0;32m  11616\u001b[0m         relu, (), \u001b[38;5;28mdict\u001b[39m(features\u001b[38;5;241m=\u001b[39mfeatures, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  11617\u001b[0m       )\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    792\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    794\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    795\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:614\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    612\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    613\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2705\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2703\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2705\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2711\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2712\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2713\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2715\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1200\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1200\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1057\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1053\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1054\u001b[0m                                          serialized)\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1057\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "# from feature_utils import generate_features, aggregate_group_prob\n",
    "# from model_utils import TARGETS, build_scaler, build_model\n",
    "# from model_utils import cv_evaluate, save_model, load_model, tune_lgb_params\n",
    "# from train_val_utils import train_validate_split, evaluate_model, evaluate_validation_set\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Log the start of the script\n",
    "logging.info(\"Script started.\")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names, but LGBMClassifier was fitted with feature names\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "def load_features(feat_dir):\n",
    "    seqs, uids = [],[]\n",
    "    feat_names = None\n",
    "    for p in Path(feat_dir).glob(\"*.csv\"):\n",
    "        df = pd.read_csv(p)\n",
    "        if feat_names is None:                      # 只抓一次欄位\n",
    "            feat_names = df.columns.tolist()\n",
    "        seqs.append(df.values)          # ← 每個 unique_id 一個時序陣列\n",
    "        uids.append(int(p.stem))\n",
    "    return seqs, np.array(uids), feat_names\n",
    "\n",
    "def prepare_train():\n",
    "    # 1. 產生特徵\n",
    "    # generate_features(\"./train_data\", \"train_info.csv\", \"tabular_data_train\")\n",
    "\n",
    "    # 2. 讀取 info & 特徵\n",
    "    info = pd.read_csv(\"train_info.csv\")\n",
    "    seqs, uid_idx, feat_names = load_features(\"tabular_data_train\")\n",
    "    groups = info.set_index(\"unique_id\").loc[uid_idx, \"player_id\"].values\n",
    "\n",
    "    # 3. 數據標準化：先攤平成 2D，再 scale，每個序列再分回\n",
    "    #    seqs: list of arrays, 每個 shape=(t_i, f)\n",
    "    all_flat = np.vstack(seqs)                      # (sum_i t_i, f)\n",
    "    scaler = build_scaler(all_flat)\n",
    "    seqs_scaled = [scaler.transform(s) for s in seqs]\n",
    "\n",
    "    # 4. 補長／裁切到同一長度\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    seq_len = max(s.shape[0] for s in seqs_scaled)\n",
    "    n_features = seqs_scaled[0].shape[1]\n",
    "    X_seq = pad_sequences(\n",
    "        seqs_scaled,\n",
    "        maxlen=seq_len,\n",
    "        dtype=\"float32\",\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=0.0\n",
    "    )  # shape = (n_sequences, seq_len, n_features)\n",
    "\n",
    "    # 5. 把這兩個放進 meta，供 build_model 使用\n",
    "    for col, meta in TARGETS.items():\n",
    "        meta[\"seq_len\"]    = seq_len\n",
    "        meta[\"n_features\"] = n_features\n",
    "    # # 儲存訓練/驗證集的分割\n",
    "    # all_targets = {}\n",
    "    # holdout_data = None\n",
    "    \n",
    "    # 4. 對每個 target 建模\n",
    "    for col, meta in TARGETS.items():\n",
    "        print(f\"\\nTraining for {col}:\")\n",
    "        logging.info(f\"\\nTraining for {col}:\")\n",
    "        y = info.set_index(\"unique_id\").loc[uid_idx, col].values\n",
    "        \n",
    "        # 拆分訓練集和驗證集\n",
    "        # data_dict = train_validate_split(X_scaled, y, groups)\n",
    "            # 👉 使用 Optuna 找最佳超參數\n",
    "        # best_params = tune_lgb_params(\n",
    "        #     data_dict[\"X_train\"], data_dict[\"y_train\"], data_dict[\"groups_train\"], meta, n_trials=20\n",
    "        # )\n",
    "        # print(f\"Best params for {col}: {best_params}\")\n",
    "        # logging.info(f\"Best params for {col}: {best_params}\")\n",
    "        # build_model 現在回傳 dict\n",
    "        out = build_model(y, meta)\n",
    "        model = out[\"model\"]\n",
    "        y_cat = out[\"y_cat\"]\n",
    "        # 拆 train / val split\n",
    "        split = int(len(X_seq) * 0.8)\n",
    "        X_tr, X_val = X_seq[:split], X_seq[split:]\n",
    "        y_tr = y_cat[:split] if y_cat.ndim > 1 else y[:split]\n",
    "        y_val = y_cat[split:] if y_cat.ndim > 1 else y[split:]        # 開始訓練\n",
    "        history = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            # callbacks=[],  # 若要 early stopping 可加\n",
    "        )\n",
    "        # 驗證結果\n",
    "        val_auc = history.history[\"val_auc\"][-1]\n",
    "        print(f\"{col} validation AUC: {val_auc:.4f}\")\n",
    "        logging.info(f\"{col} validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # mdl = build_model(y, meta)\n",
    "\n",
    "        # if holdout_data is None:\n",
    "        #     holdout_data = {\n",
    "        #         'X_val': data_dict['X_val'],\n",
    "        #         'X_train': data_dict['X_train'],\n",
    "        #         'y_val': {},\n",
    "        #         'y_train': {},\n",
    "        #         'groups_val': data_dict['groups_val']\n",
    "        #     }\n",
    "        \n",
    "        # holdout_data['y_val'][col] = data_dict['y_val']\n",
    "        # holdout_data['y_train'][col] = data_dict['y_train']\n",
    "        \n",
    "        # # 訓練和驗證\n",
    "        # # mdl = build_model(y, meta)\n",
    "        # val_score, trained_model = evaluate_model(mdl, data_dict, meta)\n",
    "        # print(f\"Training Score for {col}: {val_score:.4f}\")\n",
    "        # logging.info(f\"Training Score for {col}: {val_score:.4f}\")\n",
    "        # save_model(trained_model, scaler, feat_names, col)\n",
    "        # all_targets[col] = {'model': trained_model, 'scaler': scaler}\n",
    "        \n",
    "\n",
    "    # print(\"\\nEvaluating validation set:\")\n",
    "    # scores, avg_score = evaluate_validation_set(holdout_data, all_targets, TARGETS)\n",
    "    # np.save(\"split_uid_val.npy\", \n",
    "    #         np.unique(uid_idx[np.isin(groups, holdout_data['groups_val'])]))\n",
    "    print(\"\\n✅ Models saved to ./models/\")\n",
    "\n",
    "prepare_train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c214bffb",
   "metadata": {},
   "source": [
    "### predict_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8942707",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 437 and the array at index 659 has size 335",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    131\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredict validation data started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredict validation data finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 76\u001b[0m, in \u001b[0;36mpredict_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m     all_features\u001b[38;5;241m.\u001b[39mappend(df\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     74\u001b[0m     uid_idx\u001b[38;5;241m.\u001b[39mextend([uid] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(df))\n\u001b[1;32m---> 76\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m uid_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(uid_idx)\n\u001b[0;32m     78\u001b[0m groups \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[uid_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\User\\aicup25_racket\\.venv\\lib\\site-packages\\numpy\\_core\\shape_base.py:287\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    286\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 437 and the array at index 659 has size 335"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from feature_utils import aggregate_group_prob\n",
    "from model_utils import TARGETS, load_model\n",
    "from train_val_utils import train_validate_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"log.txt\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# Log the start of the script\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names, but LGBMClassifier was fitted with feature names\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "\n",
    "def evaluate_predictions(pred_file: str, info: pd.DataFrame):\n",
    "    pred = pd.read_csv(pred_file)\n",
    "    data = pd.merge(info, pred, on=\"unique_id\", suffixes=(\"_true\", \"_pred\"))\n",
    "\n",
    "    # 二元任務\n",
    "    gender_true = (data[\"gender_true\"] == 1).astype(int)\n",
    "    hold_true = (data[\"hold racket handed_true\"] == 1).astype(int)\n",
    "\n",
    "    gender_auc = roc_auc_score(gender_true, data[\"gender_pred\"])\n",
    "    hold_auc = roc_auc_score(hold_true, data[\"hold racket handed_pred\"])\n",
    "\n",
    "    # 多元任務\n",
    "    play_year_true = pd.get_dummies(data[\"play years\"])\n",
    "    play_year_pred = data[[f\"play years_{i}\" for i in range(3)]]\n",
    "    play_year_auc = roc_auc_score(\n",
    "        play_year_true, play_year_pred,\n",
    "        multi_class=\"ovr\", average=\"micro\"\n",
    "    )\n",
    "\n",
    "    level_true = pd.get_dummies(data[\"level\"])\n",
    "    level_pred = data[[f\"level_{i}\" for i in [2, 3, 4, 5]]]\n",
    "    level_auc = roc_auc_score(\n",
    "        level_true, level_pred,\n",
    "        multi_class=\"ovr\", average=\"micro\"\n",
    "    )\n",
    "\n",
    "    final_score = (gender_auc + hold_auc + play_year_auc + level_auc) / 4\n",
    "    print(f\"Gender ROC AUC       : {gender_auc:.4f}\")\n",
    "    print(f\"Hold Racket ROC AUC  : {hold_auc:.4f}\")\n",
    "    print(f\"Play Years ROC AUC   : {play_year_auc:.4f}\")\n",
    "    print(f\"Level ROC AUC        : {level_auc:.4f}\")\n",
    "    print(f\"Final Score          : {final_score:.4f}\")\n",
    "    return {\n",
    "        'gender': gender_auc,\n",
    "        'hold': hold_auc,\n",
    "        'play_years': play_year_auc,\n",
    "        'level': level_auc,\n",
    "        'final': final_score\n",
    "    }\n",
    "\n",
    "def predict_train():\n",
    "    # 讀取原始數據和特徵\n",
    "    info = pd.read_csv(\"train_info.csv\")\n",
    "    all_features = []\n",
    "    uid_idx = []\n",
    "    \n",
    "    for p in sorted(Path(\"tabular_data_train\").glob(\"*.csv\")):\n",
    "        uid = int(p.stem)\n",
    "        df = pd.read_csv(p)\n",
    "        all_features.append(df.values)\n",
    "        uid_idx.extend([uid] * len(df))\n",
    "    \n",
    "    X = np.vstack(all_features)\n",
    "    uid_idx = np.array(uid_idx)\n",
    "    groups = info.set_index(\"unique_id\").loc[uid_idx, \"player_id\"].values\n",
    "    \n",
    "    # 分割驗證集\n",
    "    y = info.set_index(\"unique_id\").loc[uid_idx, list(TARGETS.keys())].values\n",
    "    data_dict = train_validate_split(X, y, groups)\n",
    "    val_mask = np.isin(groups, np.unique(data_dict['groups_val']))\n",
    "    \n",
    "    # 只預測驗證集數據\n",
    "    val_uids = np.unique(uid_idx[val_mask])\n",
    "    sub_rows = []\n",
    "    \n",
    "    for uid in val_uids:\n",
    "        idx = np.where(uid_idx == uid)[0]\n",
    "        X_current = X[idx]\n",
    "        row = {\"unique_id\": uid}\n",
    "\n",
    "        for col, meta in TARGETS.items():\n",
    "            bundle = load_model(col)\n",
    "            scaler = bundle[\"scaler\"]\n",
    "            model = bundle[\"model\"]\n",
    "\n",
    "            X_scaled = scaler.transform(X_current)\n",
    "            proba = model.predict_proba(X_scaled)\n",
    "            grp = aggregate_group_prob(proba)[0]\n",
    "\n",
    "            if meta[\"type\"] == \"bin\":\n",
    "                pos_idx = np.where(model.classes_ == 1)[0][0]\n",
    "                row[col] = grp[pos_idx]\n",
    "                continue\n",
    "\n",
    "            needed_labels = [0, 1, 2] if col == \"play years\" else [2, 3, 4, 5]\n",
    "            for lbl in needed_labels:\n",
    "                row[f\"{col}_{lbl}\"] = 0.0\n",
    "\n",
    "            for idx, lbl in enumerate(model.classes_):\n",
    "                row[f\"{col}_{lbl}\"] = grp[idx]\n",
    "\n",
    "        sub_rows.append(row)\n",
    "\n",
    "    sub_cols = [\"unique_id\", \"gender\", \"hold racket handed\",\n",
    "                \"play years_0\", \"play years_1\", \"play years_2\",\n",
    "                \"level_2\", \"level_3\", \"level_4\", \"level_5\"]\n",
    "    \n",
    "    submission = pd.DataFrame(sub_rows)[sub_cols]\n",
    "    submission.to_csv(\"val_pred.csv\", index=False, float_format=\"%.8f\")\n",
    "    print(\"✅  val_pred.csv ready!\")\n",
    "    \n",
    "    # 評估驗證集分數\n",
    "    val_info = info[info['unique_id'].isin(val_uids)]\n",
    "    scores = evaluate_predictions(\"val_pred.csv\", val_info)\n",
    "    return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info(\"Predict validation data started.\")\n",
    "    scores = predict_train()\n",
    "    logging.info(\"Predict validation data finished successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae14db1",
   "metadata": {},
   "source": [
    "## Predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from feature_utils import aggregate_group_prob\n",
    "# from model_utils import TARGETS, load_model\n",
    "import pandas as pd\n",
    "def predict_test():\n",
    "    # 1. 產生特徵\n",
    "    generate_features(\"./test_data\", \"test_info.csv\", \"tabular_data_test\")\n",
    "\n",
    "    # 2. 預先加載所有模型\n",
    "    models = {col: load_model(col) for col in TARGETS.keys()}\n",
    "    \n",
    "    # 3. 批量讀取所有測試數據\n",
    "    test_files = sorted(Path(\"tabular_data_test\").glob(\"*.csv\"))\n",
    "    all_uids = []\n",
    "    all_features = []\n",
    "    \n",
    "    for p in test_files:\n",
    "        uid = int(p.stem)\n",
    "        df = pd.read_csv(p)          # ← 不要 .values\n",
    "        all_uids.append(uid)\n",
    "        all_features.append(df)      # ← 存 DataFrame\n",
    "    \n",
    "    sub_rows = []\n",
    "    for idx, uid in enumerate(all_uids):\n",
    "        X_df = all_features[idx]\n",
    "        row = {\"unique_id\": uid}\n",
    "\n",
    "        for col, meta in TARGETS.items():\n",
    "            bundle = models[col]\n",
    "            scaler = bundle[\"scaler\"]\n",
    "            model = bundle[\"model\"]\n",
    "\n",
    "            # === 對齊欄位：補缺 → 排序 ===\n",
    "            feat_order = bundle[\"feature_names\"]\n",
    "            X_aligned  = X_df.reindex(columns=feat_order, fill_value=0.0)\n",
    "\n",
    "            X_scaled = scaler.transform(X_aligned.values)\n",
    "            proba    = model.predict_proba(X_scaled)\n",
    "            grp      = aggregate_group_prob(proba)[0]\n",
    "\n",
    "            if meta[\"type\"] == \"bin\":\n",
    "                pos_idx = np.where(model.classes_ == 1)[0][0]\n",
    "                row[col] = grp[pos_idx]\n",
    "                continue\n",
    "\n",
    "            needed_labels = [0, 1, 2] if col == \"play years\" else [2, 3, 4, 5]\n",
    "            for lbl in needed_labels:\n",
    "                row[f\"{col}_{lbl}\"] = 0.0\n",
    "\n",
    "            for idx, lbl in enumerate(model.classes_):\n",
    "                row[f\"{col}_{lbl}\"] = grp[idx]\n",
    "\n",
    "        sub_rows.append(row)\n",
    "\n",
    "    sub_cols = [\"unique_id\", \"gender\", \"hold racket handed\",\n",
    "                \"play years_0\",\"play years_1\",\"play years_2\",\n",
    "                \"level_2\",\"level_3\",\"level_4\",\"level_5\"]\n",
    "    df_temp = pd.DataFrame(sub_rows)\n",
    "    df_temp = df_temp.reindex(columns=sub_cols, fill_value=0.0)\n",
    "    submission = df_temp[sub_cols]\n",
    "    # 使用 DataFrame 批量處理\n",
    "    submission.to_csv(\"submission.csv\", index=False, float_format=\"%.8f\")\n",
    "    print(\"✅  submission.csv ready!\")\n",
    "    \n",
    "predict_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
