{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e980dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "\n",
    "# 參數設定\n",
    "DATA_DIR = './train_data/'\n",
    "INFO_FILE = './train_info.csv'\n",
    "OUTPUT_DIR = './features_by_segment/train/'\n",
    "\n",
    "def spectral_features(segment: np.ndarray, fs: float = 100.0) -> list:\n",
    "    fft_vals = np.abs(rfft(segment, axis=0))\n",
    "    freqs    = rfftfreq(segment.shape[0], d=1/fs)\n",
    "    bands    = [(0,20),(20,50),(50,None)]\n",
    "    feats    = []\n",
    "    for lo, hi in bands:\n",
    "        if hi is None:\n",
    "            idx = np.where(freqs >= lo)[0]\n",
    "        else:\n",
    "            idx = np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "        if idx.size>0:\n",
    "            feats.extend(fft_vals[idx].mean(axis=0).tolist())\n",
    "        else:\n",
    "            feats.extend([0.0]*segment.shape[1])\n",
    "    return feats\n",
    "\n",
    "# 提取單一分段特徵函式\n",
    "def extract_segment_features(segment: np.ndarray) -> list:\n",
    "    # segment shape: (n_steps, 6)\n",
    "    feats = []\n",
    "    feats.extend(segment.mean(axis=0))\n",
    "    feats.extend(segment.std(axis=0))\n",
    "    feats.extend(segment.max(axis=0))\n",
    "    feats.extend(segment.min(axis=0))\n",
    "    # 偏度、峭度\n",
    "    feats.extend(skew(segment, axis=0))\n",
    "    feats.extend(kurtosis(segment, axis=0))\n",
    "    # 百分位數\n",
    "    pctls = np.percentile(segment, [10,25,50,75,90], axis=0)  # shape=(5,6)\n",
    "    feats.extend(pctls.flatten())\n",
    "    # 一階差分\n",
    "    diff = np.diff(segment, axis=0)\n",
    "    feats.extend(diff.mean(axis=0))\n",
    "    feats.extend(diff.std(axis=0))\n",
    "    # (5) —— 新增 —— 频域能量特征\n",
    "    feats.extend(spectral_features(segment, fs=100.0))\n",
    "    return feats\n",
    "\n",
    "# 生成分段級資料，一筆 row 對應一個 (unique_id, segment_id)\n",
    "# 依據 cut_point 切成多段，然後對每段計算統計特徵\n",
    "def generate_features(data_dir=DATA_DIR,\n",
    "                      info_file=INFO_FILE,\n",
    "                      output_dir=OUTPUT_DIR):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    info_df = pd.read_csv(info_file, dtype={'unique_id': str, 'cut_point': str})\n",
    "\n",
    "    all_rows = []\n",
    "    for _, row in info_df.iterrows():\n",
    "        uid = row['unique_id']\n",
    "        # 解析所有切點\n",
    "        points = list(map(int, re.findall(r\"\\d+\", row['cut_point'])))\n",
    "        # 邊界：起點 0、所有 points、最後一筆 length\n",
    "        file_path = os.path.join(data_dir, f\"{uid}.txt\")\n",
    "        df = pd.read_csv(file_path, sep='\\s+', header=None, dtype=float)\n",
    "        data = df.values\n",
    "        boundaries = [0] + points + [data.shape[0]]\n",
    "        # 逐段提取特徵\n",
    "        for seg_id in range(len(boundaries) - 1):\n",
    "            start = boundaries[seg_id]\n",
    "            end   = boundaries[seg_id + 1]\n",
    "            seg   = data[start:end]\n",
    "            if seg.size == 0:\n",
    "                feats = [0] * (4 * 6)\n",
    "            else:\n",
    "                feats = extract_segment_features(seg)\n",
    "            # 一行：unique_id, segment index, features...\n",
    "            all_rows.append([uid, seg_id] + feats)\n",
    "\n",
    "    # 組欄位名稱\n",
    "    stats = ['mean','std','max','min','skew','kurtosis']\n",
    "    pctls = ['p10','p25','p50','p75','p90']\n",
    "    diffs = ['dmean','dstd']\n",
    "    bands = ['b0_20','b20_50','b50_Nyquist']\n",
    "    cols = ['unique_id','segment_id']\n",
    "    for name_group, group in [('stat', stats), ('pct', pctls), ('diff', diffs), ('spec', bands)]:\n",
    "        for stat in group:\n",
    "            for axis in range(6):\n",
    "                cols.append(f\"{stat}_{axis}\")\n",
    "\n",
    "    features_df = pd.DataFrame(all_rows, columns=cols)\n",
    "    features_df.to_csv(os.path.join(output_dir, 'features.csv'), index=False)\n",
    "\n",
    "# 執行特徵生成\n",
    "generate_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_segments= 29  feats_per_seg= 3\n",
      "=== Shape Check ===\n",
      "X_tr: (1564, 29, 96)\n",
      "X_val: (391, 29, 96)\n",
      "y_gender   train/val: (1564, 2) (391, 2)\n",
      "y_hand     train/val: (1564, 2) (391, 2)\n",
      "y_years    train/val: (1564, 3) (391, 3)\n",
      "y_level    train/val: (1564, 4) (391, 4)\n",
      "unique_ids count: 1955\n",
      "train+val == total? 1955 vs 1955\n",
      "==========================================\n",
      "=== Label Distribution (train vs val) ===\n",
      "gender: train {0: 1287, 1: 277},  val {0: 340, 1: 51}\n",
      "hand: train {0: 1263, 1: 301},  val {0: 326, 1: 65}\n",
      "years: train {0: 307, 1: 693, 2: 564},  val {0: 80, 1: 175, 2: 136}\n",
      "level: train {0: 575, 1: 157, 2: 107, 3: 725},  val {0: 140, 1: 44, 2: 29, 3: 178}\n",
      "==========================================\n",
      "train/val 重叠的 unique_id 个数: 0\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 29, 3), found shape=(None, 29, 96)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 160\u001b[0m\n\u001b[0;32m    154\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_sub_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m    158\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_sub_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    159\u001b[0m )\n\u001b[1;32m--> 160\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myg_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myh_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myears\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myy_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlevel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myl_tr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgender\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myg_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myh_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myears\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myy_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlevel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43myl_val\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSubmissionAUC\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43myg_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myh_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myl_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\.conda\\envs\\justin\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 29, 3), found shape=(None, 29, 96)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class SubmissionAUC(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.X_val, (self.yg_val, self.yh_val, self.yy_val, self.yl_val) = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        p_g, p_h, p_y, p_l = self.model.predict(self.X_val, verbose=0)\n",
    "        y_true_g = np.argmax(self.yg_val, axis=1)  # 0=男、1=女\n",
    "        y_true_h = np.argmax(self.yh_val, axis=1)  # 0=右手、1=左手\n",
    "\n",
    "        auc_g = roc_auc_score(y_true_g, p_g[:,1])\n",
    "        auc_h = roc_auc_score(y_true_h, p_h[:,1])\n",
    "        # 多類別要用 micro-ovr\n",
    "        auc_y = roc_auc_score(self.yy_val, p_y, average='micro', multi_class='ovr')\n",
    "        auc_l = roc_auc_score(self.yl_val, p_l, average='micro', multi_class='ovr')\n",
    "        sub_auc = (auc_g + auc_h + auc_y + auc_l) / 4\n",
    "        logs = logs or {}\n",
    "        logs['val_sub_auc'] = sub_auc\n",
    "        print(f' — val_sub_auc: {sub_auc:.4f}')\n",
    "\n",
    "# ------------------ 1. 特徵生成 ------------------\n",
    "\n",
    "\n",
    "# ------------------ 2. 載入資料並合併 ------------------\n",
    "# 1. 載入分段特徵與標籤\n",
    "feat_path = './features_by_segment/train/features.csv'\n",
    "info_path = './train_info.csv'\n",
    "feat_df = pd.read_csv(feat_path, dtype={'unique_id': str})\n",
    "info_df = pd.read_csv(info_path, dtype={'unique_id': str})\n",
    "\n",
    "df = pd.merge(\n",
    "    feat_df,\n",
    "    info_df[['unique_id','gender','hold racket handed','play years','level']],\n",
    "    on='unique_id'\n",
    ")\n",
    "\n",
    "\n",
    "# 3. 構造特徵矩陣 X 與 groups\n",
    "exclude_cols = ['unique_id','segment_id','gender','hold racket handed','play years','level']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "\n",
    "feats_per_seg = len(feature_cols)\n",
    "num_segments = df['segment_id'].nunique()\n",
    "unique_ids = df['unique_id'].unique()\n",
    "\n",
    "X = np.stack([\n",
    "    df[df['unique_id']==uid]\n",
    "      .sort_values('segment_id')[feature_cols]\n",
    "      .values\n",
    "    for uid in unique_ids\n",
    "])  # shape = (n_ids, num_segments, feats_per_seg)\n",
    "# 標籤\n",
    "info_df = info_df.set_index('unique_id')\n",
    "label_df = info_df.loc[unique_ids]\n",
    "y_gender = to_categorical(label_df['gender'].values - 1, num_classes=2)\n",
    "y_hand   = to_categorical(label_df['hold racket handed'].values - 1, num_classes=2)\n",
    "y_years  = to_categorical(label_df['play years'].values, num_classes=3)\n",
    "y_level  = to_categorical(label_df['level'].values - 2, num_classes=4)\n",
    "\n",
    "# 分割訓練/驗證\n",
    "gss = GroupShuffleSplit(test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, groups=unique_ids))\n",
    "X_tr, X_val = X[train_idx], X[val_idx]\n",
    "yg_tr, yg_val = y_gender[train_idx], y_gender[val_idx]\n",
    "yh_tr, yh_val = y_hand[train_idx], y_hand[val_idx]\n",
    "yy_tr, yy_val = y_years[train_idx], y_years[val_idx]\n",
    "yl_tr, yl_val = y_level[train_idx], y_level[val_idx]\n",
    "\n",
    "# —— 1. 基本形状检查 —— #\n",
    "print(\"=== Shape Check ===\")\n",
    "print(\"X_tr:\", X_tr.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "# print(\"X_test:\", X_test.shape)\n",
    "print(\"y_gender   train/val:\", yg_tr.shape, yg_val.shape)\n",
    "print(\"y_hand     train/val:\", yh_tr.shape, yh_val.shape)\n",
    "print(\"y_years    train/val:\", yy_tr.shape, yy_val.shape)\n",
    "print(\"y_level    train/val:\", yl_tr.shape, yl_val.shape)\n",
    "print(\"unique_ids count:\", len(unique_ids))\n",
    "print(\"train+val == total?\", len(train_idx)+len(val_idx), \"vs\", len(unique_ids))\n",
    "print(\"==========================================\")\n",
    "\n",
    "# —— 5. 训练/验证标签分布 —— #\n",
    "print(\"=== Label Distribution (train vs val) ===\")\n",
    "for name, y_tr, y_v in [\n",
    "    (\"gender\", yg_tr, yg_val),\n",
    "    (\"hand\",   yh_tr, yh_val),\n",
    "    (\"years\",  yy_tr, yy_val),\n",
    "    (\"level\",  yl_tr, yl_val),\n",
    "]:\n",
    "    tr_counts = pd.Series(np.argmax(y_tr,1)).value_counts().sort_index()\n",
    "    v_counts  = pd.Series(np.argmax(y_v,1)).value_counts().sort_index()\n",
    "    print(f\"{name}: train {tr_counts.to_dict()},  val {v_counts.to_dict()}\")\n",
    "print(\"==========================================\")\n",
    "# —— 2. GroupSplit 互斥性检查 —— #\n",
    "train_uids = unique_ids[train_idx]\n",
    "val_uids   = unique_ids[val_idx]\n",
    "inter = set(train_uids) & set(val_uids)\n",
    "print(\"train/val 重叠的 unique_id 个数:\", len(inter))\n",
    "assert len(inter)==0, \"❌ 有球手同时出现在 train 和 val！\"\n",
    "\n",
    "# feats_per_seg = X.shape[2]\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr.reshape(-1, feats_per_seg)).reshape(X_tr.shape)\n",
    "X_val = scaler.transform(   X_val.reshape(-1, feats_per_seg)).reshape(X_val.shape)\n",
    "\n",
    "# ------------------ 4. RNN 模型定義 ------------------\n",
    "inputs = Input(shape=(num_segments, feats_per_seg))\n",
    "x = LSTM(64, return_sequences=False)(inputs)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "out_g = Dense(2, activation='softmax', name='gender')(x)\n",
    "out_h = Dense(2, activation='softmax', name='hand')(x)\n",
    "out_y = Dense(3, activation='softmax', name='years')(x)\n",
    "out_l = Dense(4, activation='softmax', name='level')(x)\n",
    "model = Model(inputs, [out_g, out_h, out_y, out_l])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'gender':'categorical_crossentropy',\n",
    "        'hand':'categorical_crossentropy',\n",
    "        'years':'categorical_crossentropy',\n",
    "        'level':'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'gender':'accuracy',\n",
    "        'hand':'accuracy',\n",
    "        'years':'accuracy',\n",
    "        'level':'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# ------------------ 5. 模型訓練與最佳化 ------------------\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras', monitor='val_sub_auc', mode='max', save_best_only=True\n",
    ")\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_sub_auc', mode='max', patience=3, restore_best_weights=True\n",
    ")\n",
    "model.fit(\n",
    "    X_tr,\n",
    "    {'gender':yg_tr, 'hand':yh_tr, 'years':yy_tr, 'level':yl_tr},\n",
    "    validation_data=(\n",
    "        X_val, \n",
    "        {'gender':yg_val, 'hand':yh_val, 'years':yy_val, 'level':yl_val}\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[SubmissionAUC((X_val, (yg_val, yh_val, yy_val, yl_val))), \n",
    "               checkpoint, early_stop\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811492f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Prob Sums (first 10) ===\n",
      "gender sum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "hand   sum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "years  sum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "level  sum: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "RNN_submission.csv 已生成。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 读原始分段特征\n",
    "test_feat = pd.read_csv('./features_by_segment/test/features.csv', dtype={'unique_id':str})\n",
    "\n",
    "# 2. 得到按球手顺序的 unique_id 列表\n",
    "unique_ids = test_feat['unique_id'].unique()\n",
    "\n",
    "# 3. 把分段特征 stack 成 (n_players, n_segments, feats_per_seg)\n",
    "X_test = np.stack([\n",
    "    test_feat[test_feat['unique_id']==uid]\n",
    "      .sort_values('segment_id')[feature_cols]\n",
    "      .values\n",
    "    for uid in unique_ids\n",
    "])  # (1431, num_segments, feats_per_seg)\n",
    "\n",
    "# 4. 标准化\n",
    "X_test = scaler.transform(X_test.reshape(-1, feats_per_seg)) \\\n",
    "               .reshape(X_test.shape)\n",
    "\n",
    "# 5. 预测\n",
    "# —— 4. 预测概率合法性 —— #\n",
    "p_g, p_h, p_y, p_l = model.predict(X_test, verbose=0)\n",
    "print(\"=== Prob Sums (first 10) ===\")\n",
    "print(\"gender sum:\", np.round(p_g[:10].sum(axis=1), 6))\n",
    "print(\"hand   sum:\", np.round(p_h[:10].sum(axis=1), 6))\n",
    "print(\"years  sum:\", np.round(p_y[:10].sum(axis=1), 6))\n",
    "print(\"level  sum:\", np.round(p_l[:10].sum(axis=1), 6))\n",
    "\n",
    "# 6. 用 unique_ids 构造 submission\n",
    "sub = pd.DataFrame({'unique_id': unique_ids})\n",
    "# gender: 索引0是「male」的概率\n",
    "sub['gender'] = p_g[:, 0]\n",
    "# hold racket handed: 索引0是「right」的概率\n",
    "sub['hold racket handed'] = p_h[:, 0]\n",
    "# play years 的三类\n",
    "for i in range(p_y.shape[1]):\n",
    "    sub[f'play years_{i}'] = p_y[:, i]\n",
    "# level_2、3、4、5\n",
    "for idx, lvl in enumerate([2,3,4,5]):\n",
    "    sub[f'level_{lvl}'] = p_l[:, idx]\n",
    "\n",
    "sub.to_csv('RNN_submission.csv', index=False, float_format='%.6f')\n",
    "print('RNN_submission.csv 已生成。')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "justin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
